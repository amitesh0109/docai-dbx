{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e38796c-5886-4bab-9b67-a7be99f0e2b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingest Data Into Brozne Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15dd34ee-bc65-4fcb-a884-27c56889f0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "%pip install pdfplumber python-docx\n",
    "%pip install striprtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "171e9f9d-a590-4fbf-9f44-857acaa2f4ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b12b66-04e3-4042-9fb8-e8656156d9eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, monotonically_increasing_id\n",
    "import os\n",
    "import pdfplumber\n",
    "import docx\n",
    "\n",
    "# Define DBFS path for uploaded documents (you can upload using Databricks UI)\n",
    "DOCS_PATH = \"/Volumes/docai-dbx/source_data/source_files\"\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            return \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to extract text from DOCX\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to read TXT\n",
    "def extract_text_from_txt(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Check for RTF signature\n",
    "        if content.strip().startswith(\"{\\\\rtf\"):\n",
    "            from striprtf.striprtf import rtf_to_text\n",
    "            return rtf_to_text(content)\n",
    "        else:\n",
    "            return content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Load all documents and extract text\n",
    "def load_documents(doc_dir):\n",
    "    docs = []\n",
    "    for file in os.listdir(doc_dir):\n",
    "        path = os.path.join(doc_dir, file)\n",
    "        if file.endswith(\".pdf\"):\n",
    "            text = extract_text_from_pdf(path)\n",
    "        elif file.endswith(\".docx\"):\n",
    "            text = extract_text_from_docx(path)\n",
    "        elif file.endswith(\".txt\"):\n",
    "            text = extract_text_from_txt(path)\n",
    "        else:\n",
    "            continue\n",
    "        docs.append((file, text))\n",
    "    return docs\n",
    "\n",
    "# Ingest documents\n",
    "doc_data = load_documents(DOCS_PATH)\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "df_docs = spark.createDataFrame(doc_data, [\"file_name\", \"raw_text\"])\n",
    "df_docs = df_docs.withColumn(\"doc_id\", monotonically_increasing_id())\n",
    "\n",
    "# Save to Delta Lake\n",
    "(df_docs\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(f\"`docai-dbx`.bronze.documentData\"))\n",
    "\n",
    "#print(\" Document ingestion completed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_documents",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
