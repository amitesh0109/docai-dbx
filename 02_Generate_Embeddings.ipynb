{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e447744-0361-46f1-8033-daaa98354dd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e52f78-2cd4-491f-877e-9b32afea16f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfadc078-6d92-402e-b308-d61856b44994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: 02_generate_embeddings.py\n",
    "\n",
    "from pyspark.sql.functions import col, explode, monotonically_increasing_id, udf, length, lower, trim\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa07f5f4-9fe5-41d0-a130-5821c0cad600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load raw documents from bronze layer\n",
    "df_raw = spark.read.format(\"delta\").table(f\"`docai-dbx`.bronze.documentdata\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# Create Silver layer: clean and enrich documents\n",
    "df_silver = (\n",
    "    df_raw.withColumn(\"clean_text\", trim(lower(col(\"raw_text\"))))\n",
    "           .withColumn(\"char_count\", length(\"raw_text\"))\n",
    ")\n",
    "\n",
    "# Save to Silver layer\n",
    "(df_silver.write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(f\"`docai-dbx`.silver.clean_documents\"))\n",
    "\n",
    "# Tokenizer to split into approximate chunks\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Chunking function\n",
    "@udf(ArrayType(StringType()))\n",
    "def chunk_text(text):\n",
    "    max_tokens = 500\n",
    "    if text is None:\n",
    "        return []\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_tokens):\n",
    "        chunk = \" \".join(words[i:i+max_tokens])\n",
    "        if len(chunk.strip()) > 20:\n",
    "            chunks.append(chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "df_silver = df_silver.filter(length(\"raw_text\") > 100)\n",
    "\n",
    "# Apply chunking on Silver data\n",
    "df_chunks = (df_silver\n",
    "    .withColumn(\"chunks\", chunk_text(col(\"clean_text\")))\n",
    "    .select(\"file_name\", \"doc_id\", explode(\"chunks\").alias(\"text_chunk\"))\n",
    "    .withColumn(\"chunk_id\", monotonically_increasing_id())\n",
    "    .withColumn(\"source_doc\", col(\"file_name\"))\n",
    "    .withColumn(\"chunk_len\", length(\"text_chunk\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c064e2cb-4d3f-42ed-97a1-f822a132aab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save to Gold Delta table\n",
    "(df_chunks\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"mergeSchema\", \"true\")\n",
    " .saveAsTable(f\"`docai-dbx`.gold.doc_chunks\"))\n",
    "\n",
    "print(\"âœ… Chunking and preprocessing completed. Ready for embeddings.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Generate_Embeddings",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
