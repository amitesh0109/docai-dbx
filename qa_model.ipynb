{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c461671d-46de-4ffc-8bbd-b952be3bb9be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install -q transformers sentence-transformers\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "445e7b76-5b70-420b-af03-d8437084bd85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from mlflow.pyfunc import PythonModel\n",
    "import mlflow\n",
    "\n",
    "class DocQAModel(PythonModel):\n",
    "    def load_context(self, context):\n",
    "        # Load the embedding model (same as used to create doc_embeddings table)\n",
    "        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "        # Load Hugging Face Q&A model\n",
    "        self.qa_pipeline = pipeline(\n",
    "            \"question-answering\",\n",
    "            model=\"deepset/roberta-base-squad2\",\n",
    "            tokenizer=\"deepset/roberta-base-squad2\"\n",
    "        )\n",
    "\n",
    "        # Load vector search index (replace with your actual index and endpoint)\n",
    "        self.vs_client = VectorSearchClient()\n",
    "        self.index = self.vs_client.get_index(\n",
    "            endpoint_name=\"your_vector_endpoint\",      # üîÅ Replace this\n",
    "            index_name=\"doc_embeddings_index\"          # üîÅ Replace this\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # Extract the user's question\n",
    "        question = model_input[\"question\"][0]\n",
    "\n",
    "        # Embed the question\n",
    "        question_vec = self.embedding_model.encode([question]).tolist()[0]\n",
    "\n",
    "        # Retrieve top 5 most similar chunks using vector search\n",
    "        results = self.index.similarity_search(\n",
    "            query_vector=question_vec,\n",
    "            columns=[\"chunk_id\", \"file_name\", \"text_chunk\"],\n",
    "            num_results=5\n",
    "        )\n",
    "\n",
    "        # Combine retrieved chunks into one context string\n",
    "        context_text = \"\\n\\n\".join([r[\"text_chunk\"] for r in results])\n",
    "\n",
    "        # Ask HuggingFace Q&A model\n",
    "        answer = self.qa_pipeline({\n",
    "            \"question\": question,\n",
    "            \"context\": context_text\n",
    "        })[\"answer\"]\n",
    "\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94864667-7d7d-4610-96d8-cd2701cdd4e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "qa_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
